package messaging

import (
	"context"
	"fmt"
	"sync"
	"time"

	"greatestworks/internal/infrastructure/logging"
)

// WorkerPool å·¥ä½œæ±?
type WorkerPool struct {
	workerCount int
	workQueue   chan interface{}
	workers     []*Worker
	processor   WorkerProcessor
	logger      logger.Logger
	ctx         context.Context
	cancel      context.CancelFunc
	stats       *WorkerPoolStats
	mu          sync.RWMutex
}

// WorkerProcessor å·¥ä½œå¤„ç†å™¨æ¥å?
type WorkerProcessor func(data interface{}) error

// Worker å·¥ä½œè€?
type Worker struct {
	id        int
	workQueue chan interface{}
	processor WorkerProcessor
	logger    logger.Logger
	ctx       context.Context
	stats     *WorkerStats
	mu        sync.RWMutex
}

// WorkerPoolStats å·¥ä½œæ± ç»Ÿè®¡ä¿¡æ?
type WorkerPoolStats struct {
	TotalProcessed int64                `json:"total_processed"`
	TotalFailed    int64                `json:"total_failed"`
	ActiveWorkers  int64                `json:"active_workers"`
	QueueSize      int64                `json:"queue_size"`
	StartTime      time.Time            `json:"start_time"`
	Uptime         time.Duration        `json:"uptime"`
	ByWorker       map[int]*WorkerStats `json:"by_worker"`
}

// WorkerStats å·¥ä½œè€…ç»Ÿè®¡ä¿¡æ?
type WorkerStats struct {
	ProcessedCount int64         `json:"processed_count"`
	FailedCount    int64         `json:"failed_count"`
	LastProcessed  time.Time     `json:"last_processed"`
	AvgProcessTime time.Duration `json:"avg_process_time"`
	IsActive       bool          `json:"is_active"`
}

// NewWorkerPool åˆ›å»ºå·¥ä½œæ±?
func NewWorkerPool(workerCount int, processor WorkerProcessor, logger logger.Logger) *WorkerPool {
	if workerCount <= 0 {
		workerCount = 10 // é»˜è®¤å·¥ä½œè€…æ•°é‡?
	}

	ctx, cancel := context.WithCancel(context.Background())

	pool := &WorkerPool{
		workerCount: workerCount,
		workQueue:   make(chan interface{}, workerCount*10), // é˜Ÿåˆ—å¤§å°ä¸ºå·¥ä½œè€…æ•°é‡çš„10å€?
		workers:     make([]*Worker, workerCount),
		processor:   processor,
		logger:      logger,
		ctx:         ctx,
		cancel:      cancel,
		stats: &WorkerPoolStats{
			StartTime: time.Now(),
			ByWorker:  make(map[int]*WorkerStats),
		},
	}

	// åˆ›å»ºå·¥ä½œè€?
	for i := 0; i < workerCount; i++ {
		worker := &Worker{
			id:        i + 1,
			workQueue: pool.workQueue,
			processor: processor,
			logger:    logger,
			ctx:       ctx,
			stats: &WorkerStats{
				IsActive: false,
			},
		}

		pool.workers[i] = worker
		pool.stats.ByWorker[worker.id] = worker.stats
	}

	logger.Info("Worker pool created successfully", "worker_count", workerCount, "queue_capacity", cap(pool.workQueue))
	return pool
}

// Start å¯åŠ¨å·¥ä½œæ±?
func (p *WorkerPool) Start(ctx context.Context) error {
	p.logger.Info("Starting worker pool", "worker_count", p.workerCount)

	// å¯åŠ¨æ‰€æœ‰å·¥ä½œè€?
	for _, worker := range p.workers {
		go worker.start()
	}

	// å¯åŠ¨ç»Ÿè®¡æ”¶é›†
	go p.collectStats()

	p.logger.Info("Worker pool started successfully")
	return nil
}

// Stop åœæ­¢å·¥ä½œæ±?
func (p *WorkerPool) Stop() error {
	p.logger.Info("Stopping worker pool")

	// å–æ¶ˆä¸Šä¸‹æ–‡ï¼Œåœæ­¢æ‰€æœ‰å·¥ä½œè€?
	p.cancel()

	// å…³é—­å·¥ä½œé˜Ÿåˆ—
	close(p.workQueue)

	// ç­‰å¾…æ‰€æœ‰å·¥ä½œè€…åœæ­?
	for _, worker := range p.workers {
		worker.stop()
	}

	p.logger.Info("Worker pool stopped successfully")
	return nil
}

// Submit æäº¤ä»»åŠ¡
func (p *WorkerPool) Submit(data interface{}) error {
	select {
	case p.workQueue <- data:
		p.logger.Debug("Task submitted to worker pool")
		return nil
	case <-p.ctx.Done():
		return fmt.Errorf("worker pool is stopped")
	default:
		return fmt.Errorf("worker pool queue is full")
	}
}

// SubmitWithTimeout å¸¦è¶…æ—¶çš„æäº¤ä»»åŠ¡
func (p *WorkerPool) SubmitWithTimeout(data interface{}, timeout time.Duration) error {
	ctx, cancel := context.WithTimeout(p.ctx, timeout)
	defer cancel()

	select {
	case p.workQueue <- data:
		p.logger.Debug("Task submitted to worker pool with timeout")
		return nil
	case <-ctx.Done():
		if ctx.Err() == context.DeadlineExceeded {
			return fmt.Errorf("submit timeout after %v", timeout)
		}
		return ctx.Err()
	}
}

// GetStats è·å–å·¥ä½œæ± ç»Ÿè®¡ä¿¡æ?
func (p *WorkerPool) GetStats() *WorkerPoolStats {
	p.mu.RLock()
	defer p.mu.RUnlock()

	// è®¡ç®—æ´»è·ƒå·¥ä½œè€…æ•°é‡?
	activeWorkers := int64(0)
	for _, worker := range p.workers {
		worker.mu.RLock()
		if worker.stats.IsActive {
			activeWorkers++
		}
		worker.mu.RUnlock()
	}

	// åˆ›å»ºç»Ÿè®¡ä¿¡æ¯å‰¯æœ¬
	stats := &WorkerPoolStats{
		TotalProcessed: p.stats.TotalProcessed,
		TotalFailed:    p.stats.TotalFailed,
		ActiveWorkers:  activeWorkers,
		QueueSize:      int64(len(p.workQueue)),
		StartTime:      p.stats.StartTime,
		Uptime:         time.Since(p.stats.StartTime),
		ByWorker:       make(map[int]*WorkerStats),
	}

	// å¤åˆ¶å·¥ä½œè€…ç»Ÿè®¡ä¿¡æ?
	for id, workerStats := range p.stats.ByWorker {
		stats.ByWorker[id] = &WorkerStats{
			ProcessedCount: workerStats.ProcessedCount,
			FailedCount:    workerStats.FailedCount,
			LastProcessed:  workerStats.LastProcessed,
			AvgProcessTime: workerStats.AvgProcessTime,
			IsActive:       workerStats.IsActive,
		}
	}

	return stats
}

// GetQueueSize è·å–é˜Ÿåˆ—å¤§å°
func (p *WorkerPool) GetQueueSize() int {
	return len(p.workQueue)
}

// GetWorkerCount è·å–å·¥ä½œè€…æ•°é‡?
func (p *WorkerPool) GetWorkerCount() int {
	return p.workerCount
}

// IsRunning æ£€æŸ¥å·¥ä½œæ± æ˜¯å¦è¿è¡Œä¸?
func (p *WorkerPool) IsRunning() bool {
	select {
	case <-p.ctx.Done():
		return false
	default:
		return true
	}
}

// ç§æœ‰æ–¹æ³•

// collectStats æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
func (p *WorkerPool) collectStats() {
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			stats := p.GetStats()
			p.logger.Debug("Worker pool metrics",
				"total_processed", stats.TotalProcessed,
				"total_failed", stats.TotalFailed,
				"active_workers", stats.ActiveWorkers,
				"queue_size", stats.QueueSize,
				"uptime", stats.Uptime)
		case <-p.ctx.Done():
			return
		}
	}
}

// updateStats æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
func (p *WorkerPool) updateStats(success bool) {
	p.mu.Lock()
	defer p.mu.Unlock()

	if success {
		p.stats.TotalProcessed++
	} else {
		p.stats.TotalFailed++
	}
}

// Worker æ–¹æ³•

// start å¯åŠ¨å·¥ä½œè€?
func (w *Worker) start() {
	w.logger.Debug("Worker started", "worker_id", w.id)

	for {
		select {
		case data := <-w.workQueue:
			if data != nil {
				w.processTask(data)
			}
		case <-w.ctx.Done():
			w.logger.Debug("Worker stopped", "worker_id", w.id)
			return
		}
	}
}

// stop åœæ­¢å·¥ä½œè€?
func (w *Worker) stop() {
	w.mu.Lock()
	w.stats.IsActive = false
	w.mu.Unlock()

	w.logger.Debug("Worker stopping", "worker_id", w.id)
}

// processTask å¤„ç†ä»»åŠ¡
func (w *Worker) processTask(data interface{}) {
	start := time.Now()

	// æ ‡è®°ä¸ºæ´»è·?
	w.mu.Lock()
	w.stats.IsActive = true
	w.mu.Unlock()

	defer func() {
		// æ ‡è®°ä¸ºéæ´»è·ƒ
		w.mu.Lock()
		w.stats.IsActive = false
		w.mu.Unlock()
	}()

	w.logger.Debug("Worker processing task", "worker_id", w.id)

	// å¤„ç†ä»»åŠ¡
	err := w.processor(data)
	processTime := time.Since(start)

	// æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
	w.updateStats(err == nil, processTime)

	if err != nil {
		w.logger.Error("Worker task processing failed", "error", err, "worker_id", w.id, "process_time", processTime)
	} else {
		w.logger.Debug("Worker task processed successfully", "worker_id", w.id, "process_time", processTime)
	}
}

// updateStats æ›´æ–°å·¥ä½œè€…ç»Ÿè®¡ä¿¡æ?
func (w *Worker) updateStats(success bool, processTime time.Duration) {
	w.mu.Lock()
	defer w.mu.Unlock()

	if success {
		w.stats.ProcessedCount++
		w.stats.LastProcessed = time.Now()

		// æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
		if w.stats.AvgProcessTime == 0 {
			w.stats.AvgProcessTime = processTime
		} else {
			w.stats.AvgProcessTime = (w.stats.AvgProcessTime + processTime) / 2
		}
	} else {
		w.stats.FailedCount++
	}
}

// GetStats è·å–å·¥ä½œè€…ç»Ÿè®¡ä¿¡æ?
func (w *Worker) GetStats() *WorkerStats {
	w.mu.RLock()
	defer w.mu.RUnlock()

	return &WorkerStats{
		ProcessedCount: w.stats.ProcessedCount,
		FailedCount:    w.stats.FailedCount,
		LastProcessed:  w.stats.LastProcessed,
		AvgProcessTime: w.stats.AvgProcessTime,
		IsActive:       w.stats.IsActive,
	}
}

// GetID è·å–å·¥ä½œè€…ID
func (w *Worker) GetID() int {
	return w.id
}

// IsActive æ£€æŸ¥å·¥ä½œè€…æ˜¯å¦æ´»è·?
func (w *Worker) IsActive() bool {
	w.mu.RLock()
	defer w.mu.RUnlock()
	return w.stats.IsActive
}

// å·¥ä½œæ± é…ç½?
type WorkerPoolConfig struct {
	WorkerCount     int           `json:"worker_count" yaml:"worker_count"`
	QueueSize       int           `json:"queue_size" yaml:"queue_size"`
	TaskTimeout     time.Duration `json:"task_timeout" yaml:"task_timeout"`
	EnableMetrics   bool          `json:"enable_metrics" yaml:"enable_metrics"`
	MetricsInterval time.Duration `json:"metrics_interval" yaml:"metrics_interval"`
}

// NewWorkerPoolFromConfig ä»é…ç½®åˆ›å»ºå·¥ä½œæ± 
func NewWorkerPoolFromConfig(config *WorkerPoolConfig, processor WorkerProcessor, logger logger.Logger) *WorkerPool {
	if config == nil {
		config = &WorkerPoolConfig{
			WorkerCount:     10,
			QueueSize:       100,
			TaskTimeout:     30 * time.Second,
			EnableMetrics:   true,
			MetricsInterval: 30 * time.Second,
		}
	}

	pool := NewWorkerPool(config.WorkerCount, processor, logger)

	// å¦‚æœæŒ‡å®šäº†é˜Ÿåˆ—å¤§å°ï¼Œé‡æ–°åˆ›å»ºé˜Ÿåˆ—
	if config.QueueSize > 0 {
		pool.workQueue = make(chan interface{}, config.QueueSize)
		// æ›´æ–°æ‰€æœ‰å·¥ä½œè€…çš„é˜Ÿåˆ—å¼•ç”¨
		for _, worker := range pool.workers {
			worker.workQueue = pool.workQueue
		}
	}

	logger.Info("Worker pool created from config",
		"worker_count", config.WorkerCount,
		"queue_size", config.QueueSize,
		"task_timeout", config.TaskTimeout,
		"enable_metrics", config.EnableMetrics)

	return pool
}
